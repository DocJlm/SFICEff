# network/EfficientNetB4Enhanced.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from efficientnet_pytorch import EfficientNet

class AdvancedAMSFEModule(nn.Module):
    """é«˜çº§è‡ªé€‚åº”å¤šå°ºåº¦é¢‘åŸŸå¢å¼ºæ¨¡å—"""
    def __init__(self, channels, reduction=16):
        super(AdvancedAMSFEModule, self).__init__()
        
        # å¤šå°ºåº¦é¢‘åŸŸç‰¹å¾æå–
        self.freq_branches = nn.ModuleList([
            # ä½é¢‘åˆ†æ”¯
            nn.Sequential(
                nn.Conv2d(channels, channels//4, 1),
                nn.BatchNorm2d(channels//4),
                nn.ReLU(inplace=True),
                nn.Conv2d(channels//4, channels//4, 3, padding=1),
                nn.BatchNorm2d(channels//4),
                nn.ReLU(inplace=True)
            ),
            # ä¸­é¢‘åˆ†æ”¯
            nn.Sequential(
                nn.Conv2d(channels, channels//4, 1),
                nn.BatchNorm2d(channels//4),
                nn.ReLU(inplace=True),
                nn.Conv2d(channels//4, channels//4, 3, padding=2, dilation=2),
                nn.BatchNorm2d(channels//4),
                nn.ReLU(inplace=True)
            ),
            # é«˜é¢‘åˆ†æ”¯
            nn.Sequential(
                nn.Conv2d(channels, channels//4, 1),
                nn.BatchNorm2d(channels//4),
                nn.ReLU(inplace=True),
                nn.Conv2d(channels//4, channels//4, 3, padding=4, dilation=4),
                nn.BatchNorm2d(channels//4),
                nn.ReLU(inplace=True)
            ),
            # è¶…é«˜é¢‘åˆ†æ”¯
            nn.Sequential(
                nn.Conv2d(channels, channels//4, 1),
                nn.BatchNorm2d(channels//4),
                nn.ReLU(inplace=True),
                nn.Conv2d(channels//4, channels//4, 5, padding=2),
                nn.BatchNorm2d(channels//4),
                nn.ReLU(inplace=True)
            )
        ])
        
        # è‡ªé€‚åº”æƒé‡ç”Ÿæˆ
        self.weight_generator = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, channels//reduction, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels//reduction, 4, 1),
            nn.Softmax(dim=1)
        )
        
        # ç‰¹å¾èåˆ
        self.fusion = nn.Sequential(
            nn.Conv2d(channels, channels, 1),
            nn.BatchNorm2d(channels),
            nn.ReLU(inplace=True)
        )
        
        # æ®‹å·®è¿æ¥æƒé‡
        self.residual_weight = nn.Parameter(torch.ones(1))
        
    def forward(self, x):
        # è·å–è‡ªé€‚åº”æƒé‡
        weights = self.weight_generator(x)  # B x 4 x 1 x 1
        
        # å¤šå°ºåº¦ç‰¹å¾æå–
        branch_features = []
        for i, branch in enumerate(self.freq_branches):
            feat = branch(x)
            weighted_feat = feat * weights[:, i:i+1, :, :]
            branch_features.append(weighted_feat)
        
        # ç‰¹å¾æ‹¼æ¥å’Œèåˆ
        multi_scale = torch.cat(branch_features, dim=1)
        enhanced = self.fusion(multi_scale)
        
        # æ®‹å·®è¿æ¥
        output = enhanced + self.residual_weight * x
        
        return output


class FixedCLFPFModule(nn.Module):
    """è·¨å±‚ç‰¹å¾é‡‘å­—å¡”èåˆæ¨¡å—"""
    def __init__(self, in_channels, out_channels=256):
        super(FixedCLFPFModule, self).__init__()
        
        # é€šé“å¯¹é½
        self.channel_align = nn.Conv2d(in_channels, out_channels, 1)
        
        # è®¡ç®—å„å±‚é€šé“æ•°
        mid_channels = out_channels // 2
        quarter_channels = out_channels // 4
        
        # åˆ†å±‚ç‰¹å¾æå–
        self.level1 = nn.Sequential(
            nn.Conv2d(out_channels, mid_channels, 3, padding=1),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True)
        )
        
        self.level2 = nn.Sequential(
            nn.Conv2d(mid_channels, quarter_channels, 3, padding=2, dilation=2),
            nn.BatchNorm2d(quarter_channels),
            nn.ReLU(inplace=True)
        )
        
        self.level3 = nn.Sequential(
            nn.Conv2d(quarter_channels, quarter_channels, 3, padding=4, dilation=4),
            nn.BatchNorm2d(quarter_channels),
            nn.ReLU(inplace=True)
        )
        
        # è‡ªåº•å‘ä¸Šè·¯å¾„
        self.upward_conv2 = nn.Sequential(
            nn.Conv2d(quarter_channels, mid_channels, 1),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True)
        )
        
        self.upward_conv1 = nn.Sequential(
            nn.Conv2d(mid_channels, out_channels, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(out_channels, out_channels//16, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels//16, out_channels, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        # é€šé“å¯¹é½
        x = self.channel_align(x)
        
        # è‡ªé¡¶å‘ä¸‹
        l1 = self.level1(x)
        l2 = self.level2(l1)
        l3 = self.level3(l2)
        
        # è‡ªåº•å‘ä¸Š
        up2 = self.upward_conv2(l3) + l1
        up1 = self.upward_conv1(up2) + x
        
        # æ³¨æ„åŠ›åŠ æƒ
        att = self.attention(up1)
        output = up1 * att
        
        return output


class SimplifiedSALGAModule(nn.Module):
    """ç®€åŒ–çš„è¯­ä¹‰æ„ŸçŸ¥å±€éƒ¨-å…¨å±€æ³¨æ„åŠ›æ¨¡å—"""
    def __init__(self, channels):
        super(SimplifiedSALGAModule, self).__init__()
        self.channels = channels
        
        # å±€éƒ¨æ³¨æ„åŠ›åˆ†æ”¯
        self.local_attention = nn.Sequential(
            nn.Conv2d(channels, channels//8, 1),
            nn.BatchNorm2d(channels//8),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels//8, channels, 7, padding=3, groups=channels//8),
            nn.BatchNorm2d(channels),
            nn.Sigmoid()
        )
        
        # å…¨å±€æ³¨æ„åŠ›åˆ†æ”¯
        self.global_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, channels//8, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels//8, channels, 1),
            nn.Sigmoid()
        )
        
        # é€šé“æ³¨æ„åŠ›
        self.channel_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, channels//16, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels//16, channels, 1),
            nn.Sigmoid()
        )
        
        # ç‰¹å¾èåˆ
        self.fusion = nn.Sequential(
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.BatchNorm2d(channels),
            nn.ReLU(inplace=True)
        )
        
    def forward(self, x):
        # ä¸‰ç§æ³¨æ„åŠ›
        local_att = self.local_attention(x)
        global_att = self.global_attention(x)
        channel_att = self.channel_attention(x)
        
        # ç‰¹å¾å¢å¼º
        local_feat = x * local_att
        global_feat = x * global_att
        channel_feat = x * channel_att
        
        # ç»„åˆç‰¹å¾
        combined = local_feat + global_feat + channel_feat
        
        # èåˆå’Œæ®‹å·®è¿æ¥
        output = self.fusion(combined) + x
        
        return output


class EfficientNetB4Enhanced(nn.Module):
    """EfficientNet-B4 + ä¸‰ä¸ªåˆ›æ–°ç‚¹"""
    def __init__(self, num_classes=2, drop_rate=0.3, 
                 enable_amsfe=True, enable_clfpf=True, enable_salga=True):
        super(EfficientNetB4Enhanced, self).__init__()
        
        self.num_classes = num_classes
        self.enable_amsfe = enable_amsfe
        self.enable_clfpf = enable_clfpf
        self.enable_salga = enable_salga
        
        # ä½¿ç”¨é¢„è®­ç»ƒçš„EfficientNet-B4ä½œä¸ºéª¨å¹²ç½‘ç»œ
        self.backbone = EfficientNet.from_pretrained('efficientnet-b4', num_classes=num_classes)
        
        # è·å–backboneçš„ç‰¹å¾æå–éƒ¨åˆ†ï¼ˆå»æ‰åˆ†ç±»å™¨ï¼‰
        self.features = self.backbone.extract_features
        
        # åŠ¨æ€è·å–ç‰¹å¾é€šé“æ•°
        with torch.no_grad():
            test_input = torch.randn(1, 3, 224, 224)
            test_features = self.features(test_input)
            feature_channels = test_features.shape[1]
        
        print(f"EfficientNet-B4 feature channels: {feature_channels}")
        
        current_channels = feature_channels
        
        # æ·»åŠ ä¸‰ä¸ªåˆ›æ–°æ¨¡å—
        if enable_amsfe:
            self.amsfe = AdvancedAMSFEModule(current_channels)
            print(f"AMSFE enabled with {current_channels} channels")
        
        if enable_clfpf:
            self.clfpf = FixedCLFPFModule(current_channels, 512)
            current_channels = 512
            print(f"CLFPF enabled, output channels: {current_channels}")
        
        if enable_salga:
            self.salga = SimplifiedSALGAModule(current_channels)
            print(f"SALGA enabled with {current_channels} channels")
        
        # åˆ†ç±»å™¨
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.dropout = nn.Dropout(drop_rate)
        
        # æ›´å¼ºçš„åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(current_channels, current_channels // 2),
            nn.BatchNorm1d(current_channels // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(drop_rate * 0.5),
            nn.Linear(current_channels // 2, num_classes)
        )
        
        print(f"EfficientNet-B4 Enhanced initialized:")
        print(f"  - Backbone: EfficientNet-B4 (pretrained)")
        print(f"  - Feature flow: {feature_channels} -> {current_channels} -> {num_classes}")
        print(f"  - Enhanced modules: AMSFE={enable_amsfe}, CLFPF={enable_clfpf}, SALGA={enable_salga}")
        
    def extract_features(self, x):
        """æå–éª¨å¹²ç½‘ç»œç‰¹å¾"""
        return self.features(x)
        
    def forward(self, x):
        # EfficientNetç‰¹å¾æå–
        features = self.features(x)
        
        # åº”ç”¨å¢å¼ºæ¨¡å—
        if self.enable_amsfe:
            features = self.amsfe(features)
        
        if self.enable_clfpf:
            features = self.clfpf(features)
        
        if self.enable_salga:
            features = self.salga(features)
        
        # å…¨å±€æ± åŒ–å’Œåˆ†ç±»
        features = self.global_pool(features)
        features = torch.flatten(features, 1)
        features = self.dropout(features)
        output = self.classifier(features)
        
        return output


# åˆ›å»ºæ¨¡å‹çš„å‡½æ•°
def create_enhanced_efficientnet_b4(num_classes=2, drop_rate=0.3, 
                                   enable_amsfe=True, enable_clfpf=True, enable_salga=True):
    """åˆ›å»ºå¢å¼ºç‰ˆEfficientNet-B4"""
    return EfficientNetB4Enhanced(
        num_classes=num_classes,
        drop_rate=drop_rate,
        enable_amsfe=enable_amsfe,
        enable_clfpf=enable_clfpf,
        enable_salga=enable_salga
    )


if __name__ == '__main__':
    print("ğŸš€ Testing EfficientNet-B4 Enhanced")
    
    # æµ‹è¯•ä¸åŒé…ç½®
    configs = [
        ('All modules', True, True, True),
        ('Without SALGA', True, True, False),
        ('Only AMSFE', True, False, False),
        ('Baseline', False, False, False)
    ]
    
    for name, amsfe, clfpf, salga in configs:
        print(f"\n{name}:")
        try:
            model = create_enhanced_efficientnet_b4(
                enable_amsfe=amsfe,
                enable_clfpf=clfpf,
                enable_salga=salga
            )
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            x = torch.randn(2, 3, 224, 224)
            y = model(x)
            
            # ç»Ÿè®¡å‚æ•°
            total_params = sum(p.numel() for p in model.parameters())
            
            print(f"  âœ… Output shape: {y.shape}")
            print(f"  ğŸ“Š Parameters: {total_params:,}")
            
        except Exception as e:
            print(f"  âŒ Error: {e}")
    
    print("\nğŸ‰ All tests completed!")