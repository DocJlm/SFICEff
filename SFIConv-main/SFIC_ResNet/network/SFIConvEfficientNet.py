# network/SFIConvEfficientNet.py
#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Author: Modified for EfficientNet-B4 backbone (Complete ResNet Replacement)
# Pytorch Implementation of SFIConv-EfficientNet

import torch
import torch.nn as nn
import torch.nn.functional as F
from network.SFIConv import *
import math

__all__ = ['SFIEfficientNet', 'sfi_efficientnet_b4']


def conv3x3(in_planes, out_planes, stride=1, groups=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=1, groups=groups, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


class SFIMBConv(nn.Module):
    """SFIç‰ˆæœ¬çš„MobileNet Inverted Bottleneck - å½»åº•ä¿®å¤ç‰ˆ"""
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, 
                 expand_ratio=6, alpha=0.5, se_ratio=0.25, norm_layer=None, First=False):
        super(SFIMBConv, self).__init__()
        
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
            
        self.stride = stride
        self.use_se = se_ratio is not None and se_ratio > 0
        self.use_shortcut = stride == 1 and in_channels == out_channels
        self.first = First
        self.alpha = alpha
        
        # Expansion phase
        hidden_dim = int(round(in_channels * expand_ratio))
        
        # å®Œå…¨é¿å…SFIå·ç§¯ï¼Œä½¿ç”¨æ ‡å‡†å·ç§¯
        if expand_ratio != 1:
            self.expand_conv = nn.Sequential(
                nn.Conv2d(in_channels, hidden_dim, 1, bias=False),
                norm_layer(hidden_dim),
                nn.SiLU(inplace=True)
            )
        else:
            self.expand_conv = None
            
        # Depthwise convolution - ä½¿ç”¨æ ‡å‡†åˆ†ç»„å·ç§¯
        self.dw_conv = nn.Sequential(
            nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride=stride, 
                     padding=kernel_size//2, groups=hidden_dim, bias=False),
            norm_layer(hidden_dim),
            nn.SiLU(inplace=True)
        )
        
        # SE layer
        if self.use_se:
            se_channels = max(1, int(in_channels * se_ratio))
            self.se = nn.Sequential(
                nn.AdaptiveAvgPool2d(1),
                nn.Conv2d(hidden_dim, se_channels, 1),
                nn.SiLU(inplace=True),
                nn.Conv2d(se_channels, hidden_dim, 1),
                nn.Sigmoid()
            )
        
        # Point-wise linear projection - ä½¿ç”¨æ ‡å‡†å·ç§¯
        self.pw_conv = nn.Sequential(
            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),
            norm_layer(out_channels)
        )
        
    def forward(self, x):
        # ç®€åŒ–å¤„ç†ï¼šå¦‚æœè¾“å…¥æ˜¯SFIåŒåˆ†æ”¯ï¼Œåªå–spatialåˆ†æ”¯
        if isinstance(x, tuple):
            x, _ = x
            
        identity = x if self.use_shortcut else None
        
        # Standard MobileNet block
        if self.expand_conv is not None:
            x = self.expand_conv(x)
        
        x = self.dw_conv(x)
        
        if self.use_se:
            se_weight = self.se(x)
            x = x * se_weight
            
        x = self.pw_conv(x)
        
        # Shortcut connection
        if self.use_shortcut and identity is not None:
            x = x + identity
            
        return x


class SELayer(nn.Module):
    """å¢å¼ºçš„Squeeze-and-Excitation layer for SFI features"""
    def __init__(self, channels, reduction_channels, alpha=0.5):
        super(SELayer, self).__init__()
        self.alpha = alpha
        
        # åˆ†åˆ«å¤„ç†spatialå’Œfrequencyåˆ†æ”¯
        spatial_channels = int(channels * (1 - alpha))
        freq_channels = int(channels * alpha)
        
        # ç¡®ä¿reduction_channelsä¸ä¸º0
        reduction_channels = max(1, reduction_channels)
        
        self.se_spatial = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(spatial_channels, reduction_channels, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(reduction_channels, spatial_channels, 1),
            nn.Sigmoid()
        )
        
        if freq_channels > 0:
            self.se_freq = nn.Sequential(
                nn.AdaptiveAvgPool2d(1),
                nn.Conv2d(freq_channels, reduction_channels, 1),
                nn.ReLU(inplace=True),
                nn.Conv2d(reduction_channels, freq_channels, 1),
                nn.Sigmoid()
            )
        else:
            self.se_freq = None
        
    def forward(self, x):
        x_s, x_f = x
        
        # Apply SE to spatial branch
        se_s = self.se_spatial(x_s)
        x_s = x_s * se_s
        
        # Apply SE to frequency branch
        if self.se_freq is not None and x_f.size(1) > 0:
            se_f = self.se_freq(x_f)
            x_f = x_f * se_f
        
        return x_s, x_f


class SFIEfficientNet(nn.Module):
    """SFI-EfficientNetæ¨¡å‹ - ä¸“æ³¨äºB4é…ç½®"""
    def __init__(self, variant='b4', num_classes=2, alpha=0.5, drop_rate=0.2):
        super(SFIEfficientNet, self).__init__()
        
        self.alpha = alpha
        self.drop_rate = drop_rate
        
        # EfficientNet-B4çš„ä¸“é—¨é…ç½®
        # [expand_ratio, channels, layers, stride, kernel_size, se_ratio]
        if variant == 'b4':
            width_mult = 1.4
            depth_mult = 1.8
            resolution = 380
            config = [
                [1, 24, 2, 1, 3, 0.25],   # Stage 1
                [6, 32, 4, 2, 3, 0.25],   # Stage 2  
                [6, 56, 4, 2, 5, 0.25],   # Stage 3
                [6, 112, 6, 2, 3, 0.25],  # Stage 4
                [6, 160, 6, 1, 5, 0.25],  # Stage 5
                [6, 272, 8, 2, 5, 0.25],  # Stage 6
                [6, 448, 2, 1, 3, 0.25],  # Stage 7
            ]
        else:
            raise ValueError(f"Unsupported variant: {variant}")
        
        # Scale width and depth
        config = [[c[0], int(c[1] * width_mult), max(1, int(c[2] * depth_mult)), 
                  c[3], c[4], c[5]] for c in config]
        
        # Stem - è¾“å…¥å¤„ç†
        stem_channels = int(48 * width_mult)  # B4ä½¿ç”¨48è€Œä¸æ˜¯32
        self.stem = nn.Sequential(
            nn.Conv2d(3, stem_channels, 3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(stem_channels),
            nn.SiLU(inplace=True)  # EfficientNetä½¿ç”¨SiLUè€Œä¸æ˜¯ReLU
        )
        
        # Build MBConv blocks
        self.blocks = nn.ModuleList()
        in_channels = stem_channels
        
        for stage_idx, (expand_ratio, out_channels, layers, stride, kernel_size, se_ratio) in enumerate(config):
            # First block in stage
            first_block = SFIMBConv(
                in_channels, out_channels, kernel_size, stride, 
                expand_ratio, alpha, se_ratio, First=(stage_idx==0)
            )
            self.blocks.append(first_block)
            in_channels = out_channels
            
            # Remaining blocks in stage
            for _ in range(layers - 1):
                block = SFIMBConv(
                    in_channels, out_channels, kernel_size, 1, 
                    expand_ratio, alpha, se_ratio, First=False
                )
                self.blocks.append(block)
        
        # Head - ä¹Ÿä½¿ç”¨æ ‡å‡†å·ç§¯é¿å…SFIé—®é¢˜
        final_channels = int(1792 * width_mult)  # B4çš„æœ€ç»ˆé€šé“æ•°
        self.head_conv = nn.Sequential(
            nn.Conv2d(in_channels, final_channels, 1, bias=False),
            nn.BatchNorm2d(final_channels),
            nn.SiLU(inplace=True)
        )
        
        # Global pooling and classifier
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.dropout = nn.Dropout(drop_rate) if drop_rate > 0 else nn.Identity()
        self.classifier = nn.Linear(final_channels, num_classes)
        
        # æƒé‡åˆå§‹åŒ–
        self._initialize_weights()
        
        print(f"SFI-EfficientNet-{variant.upper()} created:")
        print(f"  - Resolution: {resolution}x{resolution}")
        print(f"  - Final channels: {final_channels}")
        print(f"  - Total blocks: {len(self.blocks)}")
        print(f"  - Drop rate: {drop_rate}")
        
    def _initialize_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                init_range = 1.0 / m.weight.size(1)
                nn.init.uniform_(m.weight, -init_range, init_range)
                nn.init.zeros_(m.bias)
    
    def extract_features(self, x):
        """æå–ç‰¹å¾ï¼Œä¸è¿›è¡Œåˆ†ç±»"""
        # Stem
        x = self.stem(x)
        
        # Blocks
        for i, block in enumerate(self.blocks):
            x = block(x)
            
        # Head conv
        x = self.head_conv(x)
        
        return x
                
    def forward(self, x):
        # Feature extraction
        features = self.extract_features(x)
        
        # Global pooling
        features = self.global_pool(features)
        features = torch.flatten(features, 1)
        
        # Dropout and classification
        features = self.dropout(features)
        output = self.classifier(features)
        
        return output


def sfi_efficientnet_b4(num_classes=2, alpha=0.5, drop_rate=0.2, **kwargs):
    """åˆ›å»ºSFI-EfficientNet-B4æ¨¡å‹"""
    return SFIEfficientNet(variant='b4', num_classes=num_classes, alpha=alpha, drop_rate=drop_rate, **kwargs)


# ä¸ºäº†ä¿æŒæ¥å£ä¸€è‡´æ€§ï¼Œä¹Ÿæä¾›å…¶ä»–å˜ä½“çš„æ¥å£
def sfi_efficientnet_b0(num_classes=2, alpha=0.5, **kwargs):
    """SFI-EfficientNet-B0 (è½»é‡ç‰ˆæœ¬ç”¨äºå¿«é€Ÿæµ‹è¯•)"""
    print("Warning: B0 is deprecated, using B4 instead for better performance")
    return sfi_efficientnet_b4(num_classes=num_classes, alpha=alpha, drop_rate=0.1, **kwargs)


if __name__ == '__main__':
    # æµ‹è¯•EfficientNet-B4æ¨¡å‹
    print("Testing SFI-EfficientNet-B4...")
    
    # åˆ›å»ºæ¨¡å‹
    model = sfi_efficientnet_b4(num_classes=2).cuda()
    
    # æµ‹è¯•ä¸åŒè¾“å…¥å°ºå¯¸
    test_sizes = [224, 380, 512]  # B4æ¨è380ï¼Œä½†ä¹Ÿè¦æµ‹è¯•å…¶ä»–å°ºå¯¸
    
    for size in test_sizes:
        print(f"\nTesting with input size {size}x{size}:")
        try:
            x = torch.randn(2, 3, size, size).cuda()
            
            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                y = model(x)
                features = model.extract_features(x)
            
            print(f"  âœ… Input: {x.shape}")
            print(f"  âœ… Output: {y.shape}")
            print(f"  âœ… Features: {features.shape}")
            
        except Exception as e:
            print(f"  âŒ Error with size {size}: {e}")
    
    # ç»Ÿè®¡å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\nModel Statistics:")
    print(f"  - Total parameters: {total_params:,}")
    print(f"  - Trainable parameters: {trainable_params:,}")
    print(f"  - Model size: {total_params * 4 / 1024 / 1024:.2f} MB")
    
    print("\nğŸ‰ SFI-EfficientNet-B4 test completed successfully!")