# paper_visualization_generator.py - è®ºæ–‡çº§åˆ«å¯è§†åŒ–ç”Ÿæˆå™¨
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
from PIL import Image
import os
from torchvision import transforms
import matplotlib.patches as Rectangle
from matplotlib.gridspec import GridSpec
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥ä½ çš„å¯è§†åŒ–å·¥å…·
from visualization_tools import DeepfakeVisualizationTools
from network.MainNet import create_model

class PaperVisualizationGenerator:
    def __init__(self, model_path, device='cuda'):
        self.device = device
        self.model = self.load_model(model_path)
        self.viz_tools = DeepfakeVisualizationTools(self.model, device)
        
        # æ•°æ®è·¯å¾„
        self.real_path = "/home/zqc/FaceForensics++/c23/test/real"
        self.fake_path = "/home/zqc/FaceForensics++/c23/test/fake"
        
        # å›¾åƒé¢„å¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # è®ºæ–‡çº§åˆ«çš„é¢œè‰²é…ç½®
        self.colors = {
            'amsfe': '#FF6B6B',    # çº¢è‰²ç³» - é¢‘åŸŸ
            'clfpf': '#4ECDC4',    # é’è‰²ç³» - é‡‘å­—å¡”
            'salga': '#45B7D1',    # è“è‰²ç³» - æ³¨æ„åŠ›
            'real': '#2ECC71',     # ç»¿è‰² - çœŸå®
            'fake': '#E74C3C'      # çº¢è‰² - ä¼ªé€ 
        }
        
    def load_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        model = create_model(
            model_type='enhanced',
            num_classes=2,
            enable_amsfe=True,
            enable_clfpf=True,
            enable_salga=True
        )
        
        checkpoint = torch.load(model_path, map_location='cpu')
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        model = model.to(self.device)
        model.eval()
        return model
    
    def load_sample_images(self, num_real=3, num_fake=3):
        """åŠ è½½ä»£è¡¨æ€§æ ·æœ¬å›¾åƒ"""
        real_images = []
        fake_images = []
        
        # åŠ è½½çœŸå®å›¾åƒ
        real_files = sorted([f for f in os.listdir(self.real_path) if f.endswith('.png')])[:num_real]
        for file in real_files:
            img_path = os.path.join(self.real_path, file)
            img = Image.open(img_path).convert('RGB')
            real_images.append((img, file))
        
        # åŠ è½½ä¼ªé€ å›¾åƒ
        fake_files = sorted([f for f in os.listdir(self.fake_path) if f.endswith('.png')])[:num_fake]
        for file in fake_files:
            img_path = os.path.join(self.fake_path, file)
            img = Image.open(img_path).convert('RGB')
            fake_images.append((img, file))
        
        return real_images, fake_images
    
    def denormalize_image(self, tensor):
        """åå½’ä¸€åŒ–å›¾åƒç”¨äºæ˜¾ç¤º"""
        if len(tensor.shape) == 4:
            tensor = tensor.squeeze(0)
        
        tensor = tensor.clone()
        tensor = tensor * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + \
                 torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        tensor = torch.clamp(tensor, 0, 1)
        
        return tensor.permute(1, 2, 0).cpu().numpy()
    
    def generate_architecture_flow_figure(self, save_path='./paper_figures/architecture_flow.png'):
        """ç”Ÿæˆæ¶æ„æµç¨‹å›¾ - Figure 1"""
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # é€‰æ‹©ä¸€ä¸ªä»£è¡¨æ€§æ ·æœ¬
        real_images, fake_images = self.load_sample_images(1, 1)
        sample_img, _ = real_images[0]
        sample_tensor = self.transform(sample_img)
        
        # åˆ›å»ºå¤§å›¾å¸ƒå±€
        fig = plt.figure(figsize=(20, 12))
        gs = GridSpec(3, 6, figure=fig, height_ratios=[1, 1, 1], width_ratios=[1, 1, 1, 1, 1, 1])
        
        # è·å–æ¨¡å‹å†…éƒ¨ç‰¹å¾
        with torch.no_grad():
            _ = self.model(sample_tensor.unsqueeze(0).to(self.device))
        
        original_img = self.denormalize_image(sample_tensor)
        
        # ç¬¬ä¸€è¡Œï¼šè¾“å…¥ -> éª¨å¹²ç½‘ç»œ -> ä¸‰ä¸ªæ¨¡å—
        ax1 = fig.add_subplot(gs[0, 0])
        ax1.imshow(original_img)
        ax1.set_title('Input Image\n(Real Sample)', fontsize=12, fontweight='bold')
        ax1.axis('off')
        
        ax2 = fig.add_subplot(gs[0, 1])
        ax2.text(0.5, 0.5, 'EfficientNet-B4\nBackbone\n(Pretrained)', 
                ha='center', va='center', fontsize=11, fontweight='bold',
                bbox=dict(boxstyle="round,pad=0.3", facecolor='lightblue'))
        ax2.set_xlim(0, 1)
        ax2.set_ylim(0, 1)
        ax2.axis('off')
        
        # AMSFEæ¨¡å—å¯è§†åŒ–
        ax3 = fig.add_subplot(gs[0, 2])
        if 'amsfe' in self.viz_tools.features:
            amsfe_feat = torch.mean(self.viz_tools.features['amsfe'], dim=1).squeeze().cpu().numpy()
            amsfe_feat = cv2.resize(amsfe_feat, (224, 224))
            amsfe_feat = (amsfe_feat - amsfe_feat.min()) / (amsfe_feat.max() - amsfe_feat.min())
            im3 = ax3.imshow(amsfe_feat, cmap='Reds', alpha=0.8)
            ax3.imshow(original_img, alpha=0.3)
        ax3.set_title('AMSFE Module\n(Frequency Enhancement)', fontsize=11, fontweight='bold', color=self.colors['amsfe'])
        ax3.axis('off')
        
        # CLFPFæ¨¡å—å¯è§†åŒ–
        ax4 = fig.add_subplot(gs[0, 3])
        if 'clfpf' in self.viz_tools.features:
            clfpf_feat = torch.mean(self.viz_tools.features['clfpf'], dim=1).squeeze().cpu().numpy()
            clfpf_feat = cv2.resize(clfpf_feat, (224, 224))
            clfpf_feat = (clfpf_feat - clfpf_feat.min()) / (clfpf_feat.max() - clfpf_feat.min())
            im4 = ax4.imshow(clfpf_feat, cmap='Greens', alpha=0.8)
            ax4.imshow(original_img, alpha=0.3)
        ax4.set_title('CLFPF Module\n(Pyramid Fusion)', fontsize=11, fontweight='bold', color=self.colors['clfpf'])
        ax4.axis('off')
        
        # SALGAæ¨¡å—å¯è§†åŒ–
        ax5 = fig.add_subplot(gs[0, 4])
        if 'salga' in self.viz_tools.features:
            salga_feat = torch.mean(self.viz_tools.features['salga'], dim=1).squeeze().cpu().numpy()
            salga_feat = cv2.resize(salga_feat, (224, 224))
            salga_feat = (salga_feat - salga_feat.min()) / (salga_feat.max() - salga_feat.min())
            im5 = ax5.imshow(salga_feat, cmap='Blues', alpha=0.8)
            ax5.imshow(original_img, alpha=0.3)
        ax5.set_title('SALGA Module\n(Semantic Attention)', fontsize=11, fontweight='bold', color=self.colors['salga'])
        ax5.axis('off')
        
        # è¾“å‡º
        ax6 = fig.add_subplot(gs[0, 5])
        with torch.no_grad():
            output = self.model(sample_tensor.unsqueeze(0).to(self.device))
            prob = torch.softmax(output, dim=1)[0]
            pred_class = torch.argmax(output, dim=1).item()
        
        ax6.text(0.5, 0.5, f'Prediction:\n{"Real" if pred_class==0 else "Fake"}\nConf: {prob[pred_class]:.3f}', 
                ha='center', va='center', fontsize=11, fontweight='bold',
                bbox=dict(boxstyle="round,pad=0.3", 
                         facecolor=self.colors['real'] if pred_class==0 else self.colors['fake'],
                         alpha=0.7))
        ax6.set_xlim(0, 1)
        ax6.set_ylim(0, 1)
        ax6.axis('off')
        
        # ç¬¬äºŒè¡Œï¼šç‰¹å¾å›¾å¯è§†åŒ–
        module_names = ['amsfe', 'clfpf', 'salga']
        for i, module in enumerate(module_names):
            if module in self.viz_tools.features:
                for j in range(2):  # æ˜¾ç¤ºæ¯ä¸ªæ¨¡å—çš„2ä¸ªç‰¹å¾é€šé“
                    ax = fig.add_subplot(gs[1, i*2+j])
                    features = self.viz_tools.features[module].squeeze().cpu().numpy()
                    if j < features.shape[0]:
                        feat_map = features[j]
                        if feat_map.max() > feat_map.min():
                            feat_map = (feat_map - feat_map.min()) / (feat_map.max() - feat_map.min())
                        ax.imshow(feat_map, cmap='viridis')
                    ax.set_title(f'{module.upper()}\nCh.{j+1}', fontsize=10)
                    ax.axis('off')
        
        # ç¬¬ä¸‰è¡Œï¼šGrad-CAMå’Œç»Ÿè®¡
        ax_cam = fig.add_subplot(gs[2, :2])
        cam = self.viz_tools.grad_cam_with_batch(sample_tensor, target_layer='amsfe')
        if cam is not None:
            im_cam = ax_cam.imshow(cam, cmap='jet', alpha=0.7)
            ax_cam.imshow(original_img, alpha=0.4)
            plt.colorbar(im_cam, ax=ax_cam, fraction=0.046)
        ax_cam.set_title('Grad-CAM (AMSFE Focus)', fontsize=12, fontweight='bold')
        ax_cam.axis('off')
        
        # æ¨¡å—æ¿€æ´»ç»Ÿè®¡
        ax_stats = fig.add_subplot(gs[2, 2:4])
        module_activations = []
        module_labels = []
        for name in ['amsfe', 'clfpf', 'salga']:
            if name in self.viz_tools.features:
                activation = torch.mean(self.viz_tools.features[name]).item()
                module_activations.append(activation)
                module_labels.append(name.upper())
        
        bars = ax_stats.bar(module_labels, module_activations, 
                           color=[self.colors['amsfe'], self.colors['clfpf'], self.colors['salga']])
        ax_stats.set_title('Module Activation Strength', fontsize=12, fontweight='bold')
        ax_stats.set_ylabel('Average Activation')
        for bar, val in zip(bars, module_activations):
            ax_stats.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                         f'{val:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # æ·»åŠ ç®­å¤´è¿æ¥
        self.add_flow_arrows(fig, gs)
        
        plt.suptitle('Enhanced EfficientNet-B4 Architecture Flow for Deepfake Detection', 
                    fontsize=16, fontweight='bold', y=0.95)
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"Architecture flow figure saved to: {save_path}")
    
    def add_flow_arrows(self, fig, gs):
        """æ·»åŠ æµç¨‹ç®­å¤´"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ ç®­å¤´æ¥æ˜¾ç¤ºæ•°æ®æµ
        pass
    
    def generate_real_vs_fake_comparison(self, save_path='./paper_figures/real_vs_fake_comparison.png'):
        """ç”ŸæˆçœŸå®vsä¼ªé€ å¯¹æ¯”å›¾ - Figure 2"""
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        real_images, fake_images = self.load_sample_images(2, 2)
        
        fig, axes = plt.subplots(4, 4, figsize=(16, 16))
        
        for i in range(2):  # 2ä¸ªæ ·æœ¬
            # çœŸå®å›¾åƒ
            real_img, real_file = real_images[i]
            real_tensor = self.transform(real_img)
            
            # ä¼ªé€ å›¾åƒ
            fake_img, fake_file = fake_images[i]
            fake_tensor = self.transform(fake_img)
            
            # å¤„ç†çœŸå®å›¾åƒ
            with torch.no_grad():
                _ = self.model(real_tensor.unsqueeze(0).to(self.device))
                real_features = {k: v.clone() for k, v in self.viz_tools.features.items()}
            
            # æ˜¾ç¤ºåŸå›¾
            axes[i*2, 0].imshow(self.denormalize_image(real_tensor))
            axes[i*2, 0].set_title(f'Real Image {i+1}', fontweight='bold', color=self.colors['real'])
            axes[i*2, 0].axis('off')
            
            # æ˜¾ç¤ºå„æ¨¡å—æ³¨æ„åŠ›
            for j, module in enumerate(['amsfe', 'clfpf', 'salga']):
                if module in real_features:
                    feat = torch.mean(real_features[module], dim=1).squeeze().cpu().numpy()
                    feat = cv2.resize(feat, (224, 224))
                    feat = (feat - feat.min()) / (feat.max() - feat.min())
                    
                    axes[i*2, j+1].imshow(feat, cmap='jet', alpha=0.8)
                    axes[i*2, j+1].imshow(self.denormalize_image(real_tensor), alpha=0.3)
                    axes[i*2, j+1].set_title(f'{module.upper()}\n(Real)', fontsize=10)
                    axes[i*2, j+1].axis('off')
            
            # å¤„ç†ä¼ªé€ å›¾åƒ
            with torch.no_grad():
                _ = self.model(fake_tensor.unsqueeze(0).to(self.device))
                fake_features = {k: v.clone() for k, v in self.viz_tools.features.items()}
            
            # æ˜¾ç¤ºåŸå›¾
            axes[i*2+1, 0].imshow(self.denormalize_image(fake_tensor))
            axes[i*2+1, 0].set_title(f'Fake Image {i+1}', fontweight='bold', color=self.colors['fake'])
            axes[i*2+1, 0].axis('off')
            
            # æ˜¾ç¤ºå„æ¨¡å—æ³¨æ„åŠ›
            for j, module in enumerate(['amsfe', 'clfpf', 'salga']):
                if module in fake_features:
                    feat = torch.mean(fake_features[module], dim=1).squeeze().cpu().numpy()
                    feat = cv2.resize(feat, (224, 224))
                    feat = (feat - feat.min()) / (feat.max() - feat.min())
                    
                    axes[i*2+1, j+1].imshow(feat, cmap='jet', alpha=0.8)
                    axes[i*2+1, j+1].imshow(self.denormalize_image(fake_tensor), alpha=0.3)
                    axes[i*2+1, j+1].set_title(f'{module.upper()}\n(Fake)', fontsize=10)
                    axes[i*2+1, j+1].axis('off')
        
        plt.suptitle('Real vs Fake Image Analysis: Module-wise Attention Comparison', 
                    fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"Real vs Fake comparison saved to: {save_path}")
    
    def generate_module_effectiveness_analysis(self, save_path='./paper_figures/module_effectiveness.png'):
        """ç”Ÿæˆæ¨¡å—æœ‰æ•ˆæ€§åˆ†æ - Figure 3"""
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        real_images, fake_images = self.load_sample_images(5, 5)
        
        # ç»Ÿè®¡å„æ¨¡å—å¯¹çœŸå®å’Œä¼ªé€ å›¾åƒçš„æ¿€æ´»å·®å¼‚
        real_activations = {'amsfe': [], 'clfpf': [], 'salga': []}
        fake_activations = {'amsfe': [], 'clfpf': [], 'salga': []}
        
        # å¤„ç†çœŸå®å›¾åƒ
        for real_img, _ in real_images:
            real_tensor = self.transform(real_img)
            with torch.no_grad():
                _ = self.model(real_tensor.unsqueeze(0).to(self.device))
                for module in ['amsfe', 'clfpf', 'salga']:
                    if module in self.viz_tools.features:
                        activation = torch.mean(self.viz_tools.features[module]).item()
                        real_activations[module].append(activation)
        
        # å¤„ç†ä¼ªé€ å›¾åƒ
        for fake_img, _ in fake_images:
            fake_tensor = self.transform(fake_img)
            with torch.no_grad():
                _ = self.model(fake_tensor.unsqueeze(0).to(self.device))
                for module in ['amsfe', 'clfpf', 'salga']:
                    if module in self.viz_tools.features:
                        activation = torch.mean(self.viz_tools.features[module]).item()
                        fake_activations[module].append(activation)
        
        # åˆ›å»ºå¯è§†åŒ–
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # ä¸Šæ’ï¼šå„æ¨¡å—æ¿€æ´»å¯¹æ¯”
        modules = ['amsfe', 'clfpf', 'salga']
        module_titles = ['AMSFE (Frequency)', 'CLFPF (Pyramid)', 'SALGA (Attention)']
        
        for i, (module, title) in enumerate(zip(modules, module_titles)):
            real_vals = real_activations[module]
            fake_vals = fake_activations[module]
            
            # ç®±çº¿å›¾
            axes[0, i].boxplot([real_vals, fake_vals], labels=['Real', 'Fake'],
                              patch_artist=True,
                              boxprops=dict(facecolor=self.colors['real'], alpha=0.7),
                              medianprops=dict(color='black', linewidth=2))
            axes[0, i].set_title(f'{title}\nActivation Distribution', fontweight='bold')
            axes[0, i].set_ylabel('Activation Value')
            axes[0, i].grid(True, alpha=0.3)
        
        # ä¸‹æ’ï¼šç‰¹å¾åˆ†å¸ƒçƒ­åŠ›å›¾
        sample_real, _ = real_images[0]
        sample_fake, _ = fake_images[0]
        
        real_tensor = self.transform(sample_real)
        fake_tensor = self.transform(sample_fake)
        
        # è·å–ç‰¹å¾
        with torch.no_grad():
            _ = self.model(real_tensor.unsqueeze(0).to(self.device))
            real_features = {k: v.clone() for k, v in self.viz_tools.features.items()}
            
            _ = self.model(fake_tensor.unsqueeze(0).to(self.device))
            fake_features = {k: v.clone() for k, v in self.viz_tools.features.items()}
        
        for i, module in enumerate(modules):
            if module in real_features and module in fake_features:
                # è®¡ç®—ç‰¹å¾å·®å¼‚
                real_feat = torch.mean(real_features[module], dim=1).squeeze().cpu().numpy()
                fake_feat = torch.mean(fake_features[module], dim=1).squeeze().cpu().numpy()
                
                # è°ƒæ•´å¤§å°å¹¶è®¡ç®—å·®å¼‚
                real_feat = cv2.resize(real_feat, (56, 56))
                fake_feat = cv2.resize(fake_feat, (56, 56))
                diff = np.abs(real_feat - fake_feat)
                
                im = axes[1, i].imshow(diff, cmap='hot', interpolation='bilinear')
                axes[1, i].set_title(f'{module.upper()}\nReal-Fake Difference', fontweight='bold')
                axes[1, i].axis('off')
                plt.colorbar(im, ax=axes[1, i], fraction=0.046)
        
        plt.suptitle('Module Effectiveness Analysis: Discriminative Power Evaluation', 
                    fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"Module effectiveness analysis saved to: {save_path}")
    
    def generate_gradcam_comparison(self, save_path='./paper_figures/gradcam_comparison.png'):
        """ç”ŸæˆGrad-CAMå¯¹æ¯”åˆ†æ - Figure 4"""
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        real_images, fake_images = self.load_sample_images(2, 2)
        
        fig, axes = plt.subplots(3, 4, figsize=(16, 12))
        
        # å¯¹æ¯ä¸ªæ ·æœ¬ç”ŸæˆGrad-CAM
        samples = [(real_images[0], 'Real 1'), (fake_images[0], 'Fake 1'),
                  (real_images[1], 'Real 2'), (fake_images[1], 'Fake 2')]
        
        for col, ((img, filename), label) in enumerate(samples):
            img_tensor = self.transform(img)
            original_img = self.denormalize_image(img_tensor)
            
            # åŸå§‹å›¾åƒ
            axes[0, col].imshow(original_img)
            axes[0, col].set_title(f'{label}\n({filename})', fontweight='bold',
                                  color=self.colors['real'] if 'Real' in label else self.colors['fake'])
            axes[0, col].axis('off')
            
            # å„æ¨¡å—çš„Grad-CAM
            for row, module in enumerate(['amsfe', 'clfpf'], 1):
                cam = self.viz_tools.grad_cam_with_batch(img_tensor, target_layer=module)
                if cam is not None:
                    im = axes[row, col].imshow(cam, cmap='jet', alpha=0.7)
                    axes[row, col].imshow(original_img, alpha=0.4)
                    axes[row, col].set_title(f'{module.upper()} Grad-CAM', fontsize=10)
                    axes[row, col].axis('off')
                    
                    # åªåœ¨ç¬¬ä¸€åˆ—æ·»åŠ colorbar
                    if col == 0:
                        plt.colorbar(im, ax=axes[row, col], fraction=0.046)
        
        plt.suptitle('Grad-CAM Analysis: Model Decision Focus Comparison', 
                    fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"Grad-CAM comparison saved to: {save_path}")
    
    def generate_all_paper_figures(self, output_dir='./paper_figures'):
        """ç”Ÿæˆæ‰€æœ‰è®ºæ–‡å›¾è¡¨"""
        os.makedirs(output_dir, exist_ok=True)
        
        print("ğŸ¨ Generating comprehensive paper visualizations...")
        print("="*60)
        
        # Figure 1: æ¶æ„æµç¨‹å›¾
        print("ğŸ“Š Generating Figure 1: Architecture Flow...")
        self.generate_architecture_flow_figure(
            os.path.join(output_dir, 'figure1_architecture_flow.png')
        )
        
        # Figure 2: çœŸå®vsä¼ªé€ å¯¹æ¯”
        print("ğŸ” Generating Figure 2: Real vs Fake Comparison...")
        self.generate_real_vs_fake_comparison(
            os.path.join(output_dir, 'figure2_real_vs_fake.png')
        )
        
        # Figure 3: æ¨¡å—æœ‰æ•ˆæ€§åˆ†æ
        print("ğŸ“ˆ Generating Figure 3: Module Effectiveness...")
        self.generate_module_effectiveness_analysis(
            os.path.join(output_dir, 'figure3_module_effectiveness.png')
        )
        
        # Figure 4: Grad-CAMå¯¹æ¯”
        print("ğŸ¯ Generating Figure 4: Grad-CAM Comparison...")
        self.generate_gradcam_comparison(
            os.path.join(output_dir, 'figure4_gradcam_comparison.png')
        )
        
        print("="*60)
        print(f"âœ… All paper figures generated successfully!")
        print(f"ğŸ“ Saved to: {output_dir}")
        print("\nğŸ“‹ Generated figures:")
        print("  - figure1_architecture_flow.png (æµç¨‹æ¶æ„å›¾)")
        print("  - figure2_real_vs_fake.png (çœŸå®vsä¼ªé€ å¯¹æ¯”)")
        print("  - figure3_module_effectiveness.png (æ¨¡å—æœ‰æ•ˆæ€§åˆ†æ)")
        print("  - figure4_gradcam_comparison.png (Grad-CAMå¯¹æ¯”)")

# ä½¿ç”¨ç¤ºä¾‹
def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Starting Paper Visualization Generation")
    print("="*50)
    
    # æ¨¡å‹è·¯å¾„
    model_path = './output/efficientnet-b4-enhanced/best.pkl'
    
    if not os.path.exists(model_path):
        print(f"âŒ Model not found: {model_path}")
        print("Please ensure you have a trained model at the specified path.")
        return
    
    try:
        # åˆ›å»ºå¯è§†åŒ–ç”Ÿæˆå™¨
        generator = PaperVisualizationGenerator(model_path)
        
        # ç”Ÿæˆæ‰€æœ‰è®ºæ–‡å›¾è¡¨
        generator.generate_all_paper_figures()
        
        print("\nğŸ‰ Paper visualization generation completed!")
        print("ğŸ’¡ These figures are ready for inclusion in your research paper.")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()