# visualization_tools.py - æ·±åº¦ä¼ªé€ æ£€æµ‹å¯è§†åŒ–å·¥å…·é›†ï¼ˆå®Œå…¨ä¿®å¤ç‰ˆï¼‰
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
from PIL import Image
import os
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.metrics import roc_curve, auc, confusion_matrix
import matplotlib.patches as patches
from torchvision import transforms
import warnings
warnings.filterwarnings('ignore')

class DeepfakeVisualizationTools:
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.model.eval()
        
        # æ³¨å†Œé’©å­å‡½æ•°ç”¨äºç‰¹å¾æå–
        self.features = {}
        self.gradients = {}
        self.register_hooks()
        
    def register_hooks(self):
        """æ³¨å†Œé’©å­å‡½æ•°ç”¨äºç‰¹å¾å’Œæ¢¯åº¦æå–"""
        def get_features(name):
            def hook(model, input, output):
                self.features[name] = output.detach()
            return hook
        
        def get_gradients(name):
            def hook(model, input, output):
                def backward_hook(grad):
                    self.gradients[name] = grad
                # æ³¨å†Œæ¢¯åº¦é’©å­ï¼Œä½†åœ¨forward hookä¸­æ£€æŸ¥
                if hasattr(output, 'register_hook'):
                    try:
                        output.register_hook(backward_hook)
                    except RuntimeError:
                        # å¦‚æœæ³¨å†Œå¤±è´¥ï¼Œè·³è¿‡
                        pass
            return hook
        
        # ä¸ºå¢å¼ºæ¨¡å—æ³¨å†Œé’©å­
        hooks_registered = []
        
        if hasattr(self.model, 'amsfe'):
            self.model.amsfe.register_forward_hook(get_features('amsfe'))
            self.model.amsfe.register_forward_hook(get_gradients('amsfe'))
            hooks_registered.append('amsfe')
        
        if hasattr(self.model, 'clfpf'):
            self.model.clfpf.register_forward_hook(get_features('clfpf'))
            self.model.clfpf.register_forward_hook(get_gradients('clfpf'))
            hooks_registered.append('clfpf')
        
        if hasattr(self.model, 'salga'):
            self.model.salga.register_forward_hook(get_features('salga'))
            self.model.salga.register_forward_hook(get_gradients('salga'))
            hooks_registered.append('salga')
        
        print(f"âœ… Registered hooks for: {hooks_registered}")

    def grad_cam(self, image, target_layer='amsfe', class_idx=None):
        """ç”ŸæˆGrad-CAMçƒ­åŠ›å›¾"""
        # ä¿å­˜åŸå§‹æ¨¡å¼
        original_training_mode = self.model.training
        
        try:
            # è®¾ç½®ä¸ºevalæ¨¡å¼ï¼Œä½†å…è®¸æ¢¯åº¦è®¡ç®—
            self.model.eval()
            
            image = image.to(self.device)
            if len(image.shape) == 3:
                image = image.unsqueeze(0)
            
            # ç¡®ä¿è¾“å…¥éœ€è¦æ¢¯åº¦
            image = image.requires_grad_(True)
            
            # ä¸´æ—¶å¯ç”¨æ¢¯åº¦è®¡ç®—
            with torch.enable_grad():
                # å‰å‘ä¼ æ’­
                output = self.model(image)
                
                if class_idx is None:
                    class_idx = output.argmax(dim=1).item()
                
                # åå‘ä¼ æ’­
                self.model.zero_grad()
                class_score = output[0, class_idx]
                class_score.backward()
            
            # è·å–ç‰¹å¾å’Œæ¢¯åº¦
            if target_layer in self.features and target_layer in self.gradients:
                features = self.features[target_layer]
                gradients = self.gradients[target_layer]
                
                # è®¡ç®—æƒé‡
                weights = torch.mean(gradients, dim=(2, 3), keepdim=True)
                
                # ç”ŸæˆCAM
                cam = torch.sum(weights * features, dim=1, keepdim=True)
                cam = F.relu(cam)
                cam = F.interpolate(cam, size=(224, 224), mode='bilinear', align_corners=False)
                cam = cam.squeeze().cpu().numpy()
                
                # å½’ä¸€åŒ–
                if cam.max() > cam.min():
                    cam = (cam - cam.min()) / (cam.max() - cam.min())
                else:
                    cam = np.zeros_like(cam)
                
                return cam
            else:
                print(f"Layer {target_layer} not found in registered hooks")
                available_layers = list(self.features.keys())
                print(f"Available layers: {available_layers}")
                return None
                
        except Exception as e:
            print(f"Error in grad_cam: {e}")
            return None
        finally:
            # æ¢å¤åŸå§‹æ¨¡å¼
            self.model.train(original_training_mode)

    def grad_cam_with_batch(self, image, target_layer='amsfe', class_idx=None, batch_size=2):
        """ä½¿ç”¨æ‰¹é‡æ•°æ®ç”ŸæˆGrad-CAMçƒ­åŠ›å›¾ï¼Œé¿å…BatchNormé—®é¢˜"""
        # ä¿å­˜åŸå§‹æ¨¡å¼
        original_training_mode = self.model.training
        
        try:
            # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ä»¥é¿å…BatchNormé—®é¢˜
            self.model.train()
            
            image = image.to(self.device)
            if len(image.shape) == 3:
                image = image.unsqueeze(0)
            
            # åˆ›å»ºæ‰¹é‡æ•°æ®ï¼ˆå¤åˆ¶å›¾åƒä»¥é¿å…BatchNormé—®é¢˜ï¼‰
            batch_images = image.repeat(batch_size, 1, 1, 1)
            batch_images = batch_images.requires_grad_(True)
            
            # å‰å‘ä¼ æ’­
            output = self.model(batch_images)
            
            if class_idx is None:
                class_idx = output[0].argmax(dim=0).item()
            
            # åå‘ä¼ æ’­ï¼ˆåªå¯¹ç¬¬ä¸€å¼ å›¾åƒï¼‰
            self.model.zero_grad()
            class_score = output[0, class_idx]
            class_score.backward()
            
            # è·å–ç‰¹å¾å’Œæ¢¯åº¦
            if target_layer in self.features and target_layer in self.gradients:
                features = self.features[target_layer][0:1]  # åªå–ç¬¬ä¸€å¼ å›¾åƒçš„ç‰¹å¾
                gradients = self.gradients[target_layer][0:1]  # åªå–ç¬¬ä¸€å¼ å›¾åƒçš„æ¢¯åº¦
                
                # è®¡ç®—æƒé‡
                weights = torch.mean(gradients, dim=(2, 3), keepdim=True)
                
                # ç”ŸæˆCAM
                cam = torch.sum(weights * features, dim=1, keepdim=True)
                cam = F.relu(cam)
                cam = F.interpolate(cam, size=(224, 224), mode='bilinear', align_corners=False)
                cam = cam.squeeze().cpu().numpy()
                
                # å½’ä¸€åŒ–
                if cam.max() > cam.min():
                    cam = (cam - cam.min()) / (cam.max() - cam.min())
                else:
                    cam = np.zeros_like(cam)
                
                return cam
            else:
                print(f"Layer {target_layer} not found in registered hooks")
                available_layers = list(self.features.keys())
                print(f"Available layers: {available_layers}")
                return None
                
        except Exception as e:
            print(f"Error in grad_cam_with_batch: {e}")
            return None
        finally:
            # æ¢å¤åŸå§‹æ¨¡å¼
            self.model.train(original_training_mode)

    def visualize_attention_maps(self, image, save_path=None):
        """å¯è§†åŒ–ä¸åŒæ¨¡å—çš„æ³¨æ„åŠ›å›¾"""
        # ä¿å­˜åŸå§‹æ¨¡å¼
        original_training_mode = self.model.training
        
        try:
            self.model.eval()
            with torch.no_grad():
                _ = self.model(image.unsqueeze(0).to(self.device))
            
            fig, axes = plt.subplots(2, 3, figsize=(15, 10))
            
            # åŸå§‹å›¾åƒ
            img_np = image.permute(1, 2, 0).cpu().numpy()
            img_np = (img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)
            axes[0, 0].imshow(img_np)
            axes[0, 0].set_title('Original Image')
            axes[0, 0].axis('off')
            
            # AMSFEæ³¨æ„åŠ›å›¾
            if 'amsfe' in self.features:
                amsfe_features = self.features['amsfe']
                amsfe_attention = torch.mean(amsfe_features, dim=1).squeeze().cpu().numpy()
                amsfe_attention = cv2.resize(amsfe_attention, (224, 224))
                amsfe_attention = (amsfe_attention - amsfe_attention.min()) / (amsfe_attention.max() - amsfe_attention.min())
                
                im1 = axes[0, 1].imshow(amsfe_attention, cmap='jet', alpha=0.7)
                axes[0, 1].imshow(img_np, alpha=0.3)
                axes[0, 1].set_title('AMSFE Attention')
                axes[0, 1].axis('off')
                plt.colorbar(im1, ax=axes[0, 1], fraction=0.046)
            else:
                axes[0, 1].axis('off')
                axes[0, 1].text(0.5, 0.5, 'AMSFE\nNot Available', ha='center', va='center', transform=axes[0, 1].transAxes)
            
            # CLFPFæ³¨æ„åŠ›å›¾
            if 'clfpf' in self.features:
                clfpf_features = self.features['clfpf']
                clfpf_attention = torch.mean(clfpf_features, dim=1).squeeze().cpu().numpy()
                clfpf_attention = cv2.resize(clfpf_attention, (224, 224))
                clfpf_attention = (clfpf_attention - clfpf_attention.min()) / (clfpf_attention.max() - clfpf_attention.min())
                
                im2 = axes[0, 2].imshow(clfpf_attention, cmap='jet', alpha=0.7)
                axes[0, 2].imshow(img_np, alpha=0.3)
                axes[0, 2].set_title('CLFPF Attention')
                axes[0, 2].axis('off')
                plt.colorbar(im2, ax=axes[0, 2], fraction=0.046)
            else:
                axes[0, 2].axis('off')
                axes[0, 2].text(0.5, 0.5, 'CLFPF\nNot Available', ha='center', va='center', transform=axes[0, 2].transAxes)
            
            # SALGAæ³¨æ„åŠ›å›¾
            if 'salga' in self.features:
                salga_features = self.features['salga']
                salga_attention = torch.mean(salga_features, dim=1).squeeze().cpu().numpy()
                salga_attention = cv2.resize(salga_attention, (224, 224))
                salga_attention = (salga_attention - salga_attention.min()) / (salga_attention.max() - salga_attention.min())
                
                im3 = axes[1, 0].imshow(salga_attention, cmap='jet', alpha=0.7)
                axes[1, 0].imshow(img_np, alpha=0.3)
                axes[1, 0].set_title('SALGA Attention')
                axes[1, 0].axis('off')
                plt.colorbar(im3, ax=axes[1, 0], fraction=0.046)
            else:
                axes[1, 0].axis('off')
                axes[1, 0].text(0.5, 0.5, 'SALGA\nNot Available', ha='center', va='center', transform=axes[1, 0].transAxes)
            
            # Grad-CAM (ä½¿ç”¨æ‰¹é‡ç‰ˆæœ¬)
            cam = self.grad_cam_with_batch(image, target_layer='amsfe')
            if cam is not None:
                im4 = axes[1, 1].imshow(cam, cmap='jet', alpha=0.7)
                axes[1, 1].imshow(img_np, alpha=0.3)
                axes[1, 1].set_title('Grad-CAM (AMSFE)')
                axes[1, 1].axis('off')
                plt.colorbar(im4, ax=axes[1, 1], fraction=0.046)
            else:
                axes[1, 1].axis('off')
                axes[1, 1].text(0.5, 0.5, 'Grad-CAM\nFailed', ha='center', va='center', transform=axes[1, 1].transAxes)
            
            # ç‰¹å¾ç»Ÿè®¡å›¾
            if self.features:
                feature_stats = []
                layer_names = []
                for name, features in self.features.items():
                    if features is not None:
                        mean_activation = torch.mean(features).item()
                        feature_stats.append(mean_activation)
                        layer_names.append(name.upper())
                
                if feature_stats:
                    axes[1, 2].bar(layer_names, feature_stats, color=['red', 'green', 'blue'][:len(feature_stats)])
                    axes[1, 2].set_title('Average Feature Activation')
                    axes[1, 2].set_ylabel('Activation Value')
                    axes[1, 2].tick_params(axis='x', rotation=45)
                else:
                    axes[1, 2].axis('off')
            else:
                axes[1, 2].axis('off')
            
            plt.tight_layout()
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.show()
            
        finally:
            # æ¢å¤åŸå§‹æ¨¡å¼
            self.model.train(original_training_mode)

    def visualize_frequency_analysis(self, real_images, fake_images, save_path=None):
        """é¢‘åŸŸåˆ†æå¯è§†åŒ–"""
        fig, axes = plt.subplots(3, 4, figsize=(16, 12))
        
        for i, (real_img, fake_img) in enumerate(zip(real_images[:2], fake_images[:2])):
            # è½¬æ¢ä¸ºnumpy
            real_np = real_img.permute(1, 2, 0).cpu().numpy()
            fake_np = fake_img.permute(1, 2, 0).cpu().numpy()
            
            # å»å½’ä¸€åŒ–
            real_np = (real_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)
            fake_np = (fake_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)
            
            # è½¬æ¢ä¸ºç°åº¦å›¾
            real_gray = cv2.cvtColor((real_np * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)
            fake_gray = cv2.cvtColor((fake_np * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)
            
            # FFTå˜æ¢
            real_fft = np.fft.fft2(real_gray)
            fake_fft = np.fft.fft2(fake_gray)
            
            real_fft_shift = np.fft.fftshift(real_fft)
            fake_fft_shift = np.fft.fftshift(fake_fft)
            
            real_magnitude = np.log(np.abs(real_fft_shift) + 1)
            fake_magnitude = np.log(np.abs(fake_fft_shift) + 1)
            
            # å¯è§†åŒ–
            axes[i, 0].imshow(real_np)
            axes[i, 0].set_title(f'Real Image {i+1}')
            axes[i, 0].axis('off')
            
            axes[i, 1].imshow(fake_np)
            axes[i, 1].set_title(f'Fake Image {i+1}')
            axes[i, 1].axis('off')
            
            im1 = axes[i, 2].imshow(real_magnitude, cmap='hot')
            axes[i, 2].set_title(f'Real FFT {i+1}')
            axes[i, 2].axis('off')
            plt.colorbar(im1, ax=axes[i, 2], fraction=0.046)
            
            im2 = axes[i, 3].imshow(fake_magnitude, cmap='hot')
            axes[i, 3].set_title(f'Fake FFT {i+1}')
            axes[i, 3].axis('off')
            plt.colorbar(im2, ax=axes[i, 3], fraction=0.046)
        
        # é¢‘åŸŸå·®å¼‚åˆ†æ
        diff_magnitude = np.abs(real_magnitude - fake_magnitude)
        im3 = axes[2, 0].imshow(diff_magnitude, cmap='coolwarm')
        axes[2, 0].set_title('FFT Difference')
        axes[2, 0].axis('off')
        plt.colorbar(im3, ax=axes[2, 0], fraction=0.046)
        
        # é¢‘åŸŸç»Ÿè®¡
        axes[2, 1].hist(real_magnitude.flatten(), bins=50, alpha=0.5, label='Real', color='blue')
        axes[2, 1].hist(fake_magnitude.flatten(), bins=50, alpha=0.5, label='Fake', color='red')
        axes[2, 1].set_title('FFT Magnitude Distribution')
        axes[2, 1].legend()
        
        # éšè—å‰©ä½™å­å›¾
        axes[2, 2].axis('off')
        axes[2, 3].axis('off')
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

    def visualize_feature_maps(self, image, layer_names=['amsfe', 'clfpf', 'salga'], save_path=None):
        """å¯è§†åŒ–ä¸åŒå±‚çš„ç‰¹å¾å›¾"""
        # ä¿å­˜åŸå§‹æ¨¡å¼
        original_training_mode = self.model.training
        
        try:
            self.model.eval()
            with torch.no_grad():
                _ = self.model(image.unsqueeze(0).to(self.device))
            
            available_layers = [name for name in layer_names if name in self.features]
            
            if not available_layers:
                print("No feature layers available for visualization")
                return
            
            num_layers = len(available_layers)
            fig, axes = plt.subplots(num_layers, 8, figsize=(20, 3*num_layers))
            
            if num_layers == 1:
                axes = axes.reshape(1, -1)
            
            for i, layer_name in enumerate(available_layers):
                features = self.features[layer_name].squeeze().cpu().numpy()
                
                # é€‰æ‹©å‰8ä¸ªé€šé“è¿›è¡Œå¯è§†åŒ–
                num_channels = min(8, features.shape[0])
                
                for j in range(num_channels):
                    feature_map = features[j]
                    # å®‰å…¨çš„å½’ä¸€åŒ–
                    if feature_map.max() > feature_map.min():
                        feature_map = (feature_map - feature_map.min()) / (feature_map.max() - feature_map.min())
                    else:
                        feature_map = np.zeros_like(feature_map)
                    
                    axes[i, j].imshow(feature_map, cmap='viridis')
                    axes[i, j].set_title(f'{layer_name.upper()} Ch.{j+1}')
                    axes[i, j].axis('off')
                
                # éšè—å¤šä½™çš„å­å›¾
                for j in range(num_channels, 8):
                    axes[i, j].axis('off')
            
            plt.tight_layout()
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.show()
            
        finally:
            # æ¢å¤åŸå§‹æ¨¡å¼
            self.model.train(original_training_mode)

    def visualize_roc_curves(self, y_true_dict, y_scores_dict, save_path=None):
        """å¯è§†åŒ–å¤šä¸ªæ¨¡å‹çš„ROCæ›²çº¿å¯¹æ¯”"""
        plt.figure(figsize=(10, 8))
        
        colors = ['blue', 'red', 'green', 'orange', 'purple']
        
        for i, (model_name, y_scores) in enumerate(y_scores_dict.items()):
            y_true = y_true_dict[model_name]
            fpr, tpr, _ = roc_curve(y_true, y_scores)
            roc_auc = auc(fpr, tpr)
            
            plt.plot(fpr, tpr, color=colors[i % len(colors)], lw=2,
                    label=f'{model_name} (AUC = {roc_auc:.3f})')
        
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.5)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate', fontsize=12)
        plt.ylabel('True Positive Rate', fontsize=12)
        plt.title('ROC Curves Comparison', fontsize=14)
        plt.legend(loc="lower right", fontsize=11)
        plt.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

    def visualize_tsne_features(self, features_dict, labels_dict, save_path=None):
        """ä½¿ç”¨t-SNEå¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒ"""
        fig, axes = plt.subplots(1, len(features_dict), figsize=(5*len(features_dict), 5))
        
        if len(features_dict) == 1:
            axes = [axes]
        
        for i, (model_name, features) in enumerate(features_dict.items()):
            labels = labels_dict[model_name]
            
            # t-SNEé™ç»´
            tsne = TSNE(n_components=2, random_state=42, perplexity=30)
            features_2d = tsne.fit_transform(features)
            
            # ç»˜åˆ¶æ•£ç‚¹å›¾
            scatter = axes[i].scatter(features_2d[:, 0], features_2d[:, 1], 
                                    c=labels, cmap='RdYlBu', alpha=0.7, s=20)
            axes[i].set_title(f't-SNE: {model_name}', fontsize=12)
            axes[i].set_xlabel('t-SNE 1')
            axes[i].set_ylabel('t-SNE 2')
            
            # æ·»åŠ é¢œè‰²æ¡
            cbar = plt.colorbar(scatter, ax=axes[i])
            cbar.set_label('Real (0) / Fake (1)')
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

    def visualize_confusion_matrices(self, y_true_dict, y_pred_dict, save_path=None):
        """å¯è§†åŒ–æ··æ·†çŸ©é˜µå¯¹æ¯”"""
        num_models = len(y_true_dict)
        fig, axes = plt.subplots(1, num_models, figsize=(5*num_models, 4))
        
        if num_models == 1:
            axes = [axes]
        
        for i, (model_name, y_true) in enumerate(y_true_dict.items()):
            y_pred = y_pred_dict[model_name]
            cm = confusion_matrix(y_true, y_pred)
            
            # å½’ä¸€åŒ–
            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            
            # ç»˜åˆ¶çƒ­åŠ›å›¾
            sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',
                       xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'],
                       ax=axes[i])
            axes[i].set_title(f'Confusion Matrix: {model_name}')
            axes[i].set_xlabel('Predicted')
            axes[i].set_ylabel('Actual')
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

    def visualize_detection_examples(self, test_loader, num_examples=8, save_path=None):
        """å¯è§†åŒ–æ£€æµ‹ç»“æœç¤ºä¾‹"""
        # ä¿å­˜åŸå§‹æ¨¡å¼
        original_training_mode = self.model.training
        
        try:
            self.model.eval()
            
            fig, axes = plt.subplots(2, num_examples, figsize=(2*num_examples, 6))
            
            correct_count = 0
            wrong_count = 0
            
            with torch.no_grad():
                for images, labels in test_loader:
                    images = images.to(self.device)
                    outputs = self.model(images)
                    predictions = torch.softmax(outputs, dim=1)
                    predicted_classes = torch.argmax(outputs, dim=1)
                    
                    for i, (image, label, pred_class, pred_prob) in enumerate(
                        zip(images, labels, predicted_classes, predictions)):
                        
                        if correct_count < num_examples//2 and pred_class == label:
                            # æ­£ç¡®é¢„æµ‹çš„ä¾‹å­
                            img_np = image.permute(1, 2, 0).cpu().numpy()
                            img_np = (img_np * np.array([0.229, 0.224, 0.225]) + 
                                     np.array([0.485, 0.456, 0.406])).clip(0, 1)
                            
                            axes[0, correct_count].imshow(img_np)
                            axes[0, correct_count].set_title(
                                f'âœ“ True: {"Fake" if label==1 else "Real"}\n'
                                f'Pred: {"Fake" if pred_class==1 else "Real"} '
                                f'({pred_prob[pred_class]:.3f})', 
                                color='green', fontsize=10)
                            axes[0, correct_count].axis('off')
                            correct_count += 1
                        
                        elif wrong_count < num_examples//2 and pred_class != label:
                            # é”™è¯¯é¢„æµ‹çš„ä¾‹å­
                            img_np = image.permute(1, 2, 0).cpu().numpy()
                            img_np = (img_np * np.array([0.229, 0.224, 0.225]) + 
                                     np.array([0.485, 0.456, 0.406])).clip(0, 1)
                            
                            axes[1, wrong_count].imshow(img_np)
                            axes[1, wrong_count].set_title(
                                f'âœ— True: {"Fake" if label==1 else "Real"}\n'
                                f'Pred: {"Fake" if pred_class==1 else "Real"} '
                                f'({pred_prob[pred_class]:.3f})', 
                                color='red', fontsize=10)
                            axes[1, wrong_count].axis('off')
                            wrong_count += 1
                        
                        if correct_count >= num_examples//2 and wrong_count >= num_examples//2:
                            break
                    
                    if correct_count >= num_examples//2 and wrong_count >= num_examples//2:
                        break
            
            # éšè—å¤šä½™çš„å­å›¾
            for j in range(correct_count, num_examples):
                axes[0, j].axis('off')
            for j in range(wrong_count, num_examples):
                axes[1, j].axis('off')
            
            axes[0, 0].set_ylabel('Correct Predictions', fontsize=12, rotation=90)
            axes[1, 0].set_ylabel('Wrong Predictions', fontsize=12, rotation=90)
            
            plt.tight_layout()
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.show()
            
        finally:
            # æ¢å¤åŸå§‹æ¨¡å¼
            self.model.train(original_training_mode)

    def create_comprehensive_visualization_report(self, test_loader, save_dir='./visualizations'):
        """ç”Ÿæˆå®Œæ•´çš„å¯è§†åŒ–æŠ¥å‘Š"""
        os.makedirs(save_dir, exist_ok=True)
        
        print("ğŸ¨ Generating comprehensive visualization report...")
        
        # 1. æ£€æµ‹ç»“æœç¤ºä¾‹
        print("ğŸ“¸ Creating detection examples...")
        self.visualize_detection_examples(
            test_loader, 
            save_path=os.path.join(save_dir, 'detection_examples.png')
        )
        
        # 2. æ³¨æ„åŠ›å¯è§†åŒ–
        print("ğŸ” Creating attention visualizations...")
        sample_batch = next(iter(test_loader))
        sample_image = sample_batch[0][0]  # å–ç¬¬ä¸€å¼ å›¾åƒ
        
        self.visualize_attention_maps(
            sample_image,
            save_path=os.path.join(save_dir, 'attention_maps.png')
        )
        
        # 3. ç‰¹å¾å›¾å¯è§†åŒ–
        print("ğŸ—ºï¸ Creating feature maps...")
        self.visualize_feature_maps(
            sample_image,
            save_path=os.path.join(save_dir, 'feature_maps.png')
        )
        
        print(f"âœ… Visualization report saved to {save_dir}")

# ä½¿ç”¨ç¤ºä¾‹ä»£ç 
def main():
    """å¯è§†åŒ–å·¥å…·ä½¿ç”¨ç¤ºä¾‹"""
    print("ğŸ¨ Starting Deepfake Visualization Tools")
    print("="*50)
    
    try:
        # å¯¼å…¥æ¨¡å—
        from network.MainNet import create_model
        
        # åˆ›å»ºæ¨¡å‹
        print("ğŸ“¦ Creating Enhanced EfficientNet-B4 model...")
        model = create_model(
            model_type='enhanced', 
            num_classes=2,
            enable_amsfe=True, 
            enable_clfpf=True, 
            enable_salga=True
        )
        
        # åŠ è½½ä½ å·²è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡
        model_path = './output/efficientnet-b4-enhanced/best.pkl'
        
        if os.path.exists(model_path):
            print(f"ğŸ“‚ Loading trained model from: {model_path}")
            try:
                # å°è¯•åŠ è½½æƒé‡
                checkpoint = torch.load(model_path, map_location='cpu')
                
                # å¤„ç†ä¸åŒçš„ä¿å­˜æ ¼å¼
                if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                    print("âœ… Loaded from checkpoint format")
                else:
                    model.load_state_dict(checkpoint)
                    print("âœ… Loaded from state dict format")
                
                print("ğŸ¯ Model weights loaded successfully!")
                
            except Exception as e:
                print(f"âš ï¸ Error loading weights: {e}")
                print("ğŸ“ Continuing with random weights for demonstration")
        else:
            print(f"âŒ Model file not found: {model_path}")
            print("ğŸ’¡ Available model files:")
            # åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹æ–‡ä»¶
            for root, dirs, files in os.walk('./output'):
                for file in files:
                    if file.endswith('.pkl') or file.endswith('.pth'):
                        print(f"   - {os.path.join(root, file)}")
            
            print("ğŸ“ Using random weights for demonstration")
        
        # åˆ›å»ºå¯è§†åŒ–å·¥å…·
        print("ğŸ”§ Initializing visualization tools...")
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        viz_tools = DeepfakeVisualizationTools(model, device=device)
        
        print(f"ğŸ–¥ï¸ Using device: {device}")
        
        # æµ‹è¯•å¯è§†åŒ–åŠŸèƒ½
        print("\nğŸ§ª Testing visualization capabilities...")
        
        # åˆ›å»ºæµ‹è¯•ç›®å½•
        test_dir = './test_visualizations'
        os.makedirs(test_dir, exist_ok=True)
        
        # 1. æ¨¡å‹ç»“æ„åˆ†æ
        print("ğŸ“Š Analyzing model structure...")
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"   Total parameters: {total_params:,}")
        print(f"   Trainable parameters: {trainable_params:,}")
        print(f"   Model size: {total_params * 4 / 1024 / 1024:.2f} MB")
        
        # 2. æµ‹è¯•å‰å‘ä¼ æ’­
        print("ğŸ”„ Testing forward pass...")
        test_input = torch.randn(1, 3, 224, 224)
        if torch.cuda.is_available():
            test_input = test_input.cuda()
            model = model.cuda()
        
        model.eval()
        with torch.no_grad():
            output = model(test_input)
        print(f"âœ… Forward pass successful! Output shape: {output.shape}")
        
        # 3. ç”Ÿæˆéšæœºæµ‹è¯•å›¾åƒè¿›è¡Œå¯è§†åŒ–
        print("ğŸ–¼ï¸ Generating test visualizations...")
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_image = torch.randn(3, 224, 224)
        
        # æµ‹è¯•æ³¨æ„åŠ›å›¾ç”Ÿæˆ
        try:
            print("   ğŸ” Generating attention maps...")
            viz_tools.visualize_attention_maps(
                test_image, 
                save_path=os.path.join(test_dir, 'attention_maps_test.png')
            )
            print("   âœ… Attention maps generated")
        except Exception as e:
            print(f"   âŒ Attention maps failed: {e}")
        
        # æµ‹è¯•ç‰¹å¾å›¾å¯è§†åŒ–
        try:
            print("   ğŸ—ºï¸ Generating feature maps...")
            viz_tools.visualize_feature_maps(
                test_image,
                save_path=os.path.join(test_dir, 'feature_maps_test.png')
            )
            print("   âœ… Feature maps generated")
        except Exception as e:
            print(f"   âŒ Feature maps failed: {e}")
        
        # æµ‹è¯•Grad-CAMï¼ˆä½¿ç”¨æ‰¹é‡ç‰ˆæœ¬ï¼‰
        try:
            print("   ğŸ¯ Generating Grad-CAM...")
            cam = viz_tools.grad_cam_with_batch(test_image, target_layer='amsfe')
            if cam is not None:
                plt.figure(figsize=(8, 6))
                plt.imshow(cam, cmap='jet')
                plt.title('Grad-CAM Visualization (Batch Version)')
                plt.colorbar()
                plt.axis('off')
                plt.savefig(os.path.join(test_dir, 'gradcam_test.png'), dpi=300, bbox_inches='tight')
                plt.close()
                print("   âœ… Grad-CAM generated")
            else:
                print("   âš ï¸ Grad-CAM generation failed")
        except Exception as e:
            print(f"   âŒ Grad-CAM failed: {e}")
        
        # 4. æ¨¡å‹å±‚çº§åˆ†æ
        print("ğŸ” Analyzing model layers...")
        available_features = list(viz_tools.features.keys()) if hasattr(viz_tools, 'features') else []
        print(f"   Available feature layers: {available_features}")
        
        # åˆ†æå„æ¨¡å—å‚æ•°
        print("ğŸ“ˆ Module parameter analysis:")
        for name, module in model.named_children():
            if hasattr(module, 'parameters'):
                params = sum(p.numel() for p in module.parameters())
                if params > 0:
                    percentage = params / total_params * 100
                    print(f"   {name:15s}: {params:>10,} ({percentage:5.1f}%)")
        
        print(f"\nğŸ‰ Visualization tools test completed!")
        print(f"ğŸ“ Test results saved to: {test_dir}")
        
        # æ˜¾ç¤ºå¯ç”¨çš„æ–¹æ³•
        print(f"\nğŸ“‹ Available visualization methods:")
        print(f"   - viz_tools.visualize_attention_maps(image)")
        print(f"   - viz_tools.visualize_feature_maps(image)")
        print(f"   - viz_tools.grad_cam(image, target_layer='amsfe')")
        print(f"   - viz_tools.grad_cam_with_batch(image, target_layer='amsfe')  # æ¨è")
        print(f"   - viz_tools.visualize_frequency_analysis(real_imgs, fake_imgs)")
        print(f"   - viz_tools.visualize_roc_curves(y_true_dict, y_scores_dict)")
        print(f"   - viz_tools.visualize_tsne_features(features_dict, labels_dict)")
        print(f"   - viz_tools.visualize_confusion_matrices(y_true_dict, y_pred_dict)")
        
        # å¦‚æœéœ€è¦åŠ è½½çœŸå®æ•°æ®è¿›è¡Œæµ‹è¯•
        print(f"\nğŸ’¡ To use with real data:")
        print(f"   1. Load your test dataset")
        print(f"   2. Call viz_tools.create_comprehensive_visualization_report(test_loader)")
        
        # ç¤ºä¾‹ï¼šå¦‚ä½•ä½¿ç”¨çœŸå®æ•°æ®
        print(f"\nğŸ“– Example usage with real data:")
        print(f"   # Load test data")
        print(f"   from network.data import TestDataset")
        print(f"   from torch.utils.data import DataLoader")
        print(f"   from torchvision import transforms")
        print(f"   ")
        print(f"   transform = transforms.Compose([")
        print(f"       transforms.Resize((224, 224)),")
        print(f"       transforms.ToTensor(),")
        print(f"       transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])")
        print(f"   ])")
        print(f"   ")
        print(f"   test_dataset = TestDataset('test.txt', test_transform=transform)")
        print(f"   test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)")
        print(f"   ")
        print(f"   # Generate comprehensive report")
        print(f"   viz_tools.create_comprehensive_visualization_report(test_loader)")
        
        # é¢å¤–æç¤º
        print(f"\nğŸ”§ Advanced features:")
        print(f"   - Use grad_cam_with_batch() instead of grad_cam() to avoid BatchNorm issues")
        print(f"   - All visualizations automatically handle model mode switching")
        print(f"   - Feature extraction works with your trained EfficientNet-B4 Enhanced model")
        
        return viz_tools
        
    except ImportError as e:
        print(f"âŒ Import error: {e}")
        print("ğŸ’¡ Please make sure all required modules are available")
        print("   Try: pip install matplotlib seaborn opencv-python scikit-learn")
        return None
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()